# -*- coding: utf-8 -*-
"""automate_Ghifari-Fikri-Yulistia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15cXFlaFWi2pvvsldyx--wW9N2W3joE1O
"""

import pandas as pd
import nltk
import os
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def process_text_data(df):
    df = df.drop_duplicates()
    n = df['spam'].value_counts().min()
    df_balanced = df.sample(frac=1).groupby('spam').head(n)

    x = df_balanced['text'].astype(str).str.lower()
    y = df_balanced['spam'].astype(float)
    x = x.apply(lambda text: " ".join([word for word in text.split() if word not in stop_words]))

    tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer.fit_on_texts(x)
    x_sequences = tokenizer.texts_to_sequences(x)

    vectorizer = TfidfVectorizer(max_features=3000)
    X_tfidf = vectorizer.fit_transform(x)

    return x_sequences, y, X_tfidf, tokenizer, vectorizer

if __name__ == "__main__":
    input_path = "spam ham emails_raw/spam ham emails.csv"
    output_path = "preprocessing/spam ham emails_preprocessing/"
    os.makedirs(output_path, exist_ok=True)

    df = pd.read_csv(input_path)

    x_seq, y, X_tfidf, tokenizer, vectorizer = process_text_data(df)

    pd.DataFrame({'label': y}).to_csv(os.path.join(output_path, "labels.csv"), index=False)
    pd.DataFrame(X_tfidf.toarray()).to_csv(os.path.join(output_path, "tfidf.csv"), index=False)

    with open(os.path.join(output_path, "tokenizer.pkl"), "wb") as f:
        pickle.dump(tokenizer, f)
    with open(os.path.join(output_path, "vectorizer.pkl"), "wb") as f:
        pickle.dump(vectorizer, f)

    print("âœ… Preprocessing selesai. Data disimpan di:", output_path)